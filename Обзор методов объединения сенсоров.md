---
tags:
  - Sensor_Fusion
type: page
---

## Введение

Рассмотрены некоторые популярные и наиболее распространенные модели и алгоритмы объединения данных с различных сенсоров (фьюзинг или fusion). Фокус был отдан объединению лидаров и камер, но радар в некоротых подходах так же может использоваться.   

## Основные подходы

Ниже представлено приблизительное деление методов фьюзинга. Деление очень условное, многие подходы частично можно отнести как к одному так и другому подходу.    
![](attachments/f4143972703c3b99bdf006bc70070146.png)    

### Early Fusion (Sensor level)

Объединение на уровне "сырых данных" сенсоров или с минимальной обработкой. Один на всех энкодер и декодер. Самый простой сценарий - добавить к лидарным точкам цвета или к пикселям расстояния от тех точек лидара, которые совпали на какой-то проекции.   

### Middle Fusion (Embeding level)

Объединение на уровне промежуточных карт признаков каждого энкодера или слоев внимания. Например картиночный и лидарные энкодеры и единый декодер. Обучение нужно проводить только на парах лидар-камера.   

### Late Fusion (Result level)

Объединение на уровне 3Д-объектов, полученных от каждой модели. Крайний случай - модели полноценно отрабатываю до получения детекций с каждой. Однако, например, 2Д-детектор может использоваться в режиме лишь генератора регионов интереса, с заниженным порогом.   

### Сравнительная таблица подходов

|                                                                        | Early | Middle | Late |
|:-----------------------------------------------------------------------|:------|:-------|:-----|
|                                                   Простота архитектуры |   +++ |     ++ |    + |
|                                                      Экономия ресурсов |   +++ |     ++ |    + |
|                                                     Интерпретируемость |     + |     ++ |  +++ |
|                                                      Гибкость обучения |     + |     ++ |  +++ |
|                                           Монолитность запуска в проде |   +++ |    +++ |    + |
|                                   Использование мультимодальных знаний |     + |    +++ |   ++ |

## Способы проецирования

Ниже предстатвлены четыре наиболее часто встречающихся способа привести различные по своей природе сенсорные данные к единому представлению. Основная идея - трансформировать данные сенсоров таким образом, чтобы точки лидара и пиксели камеры, указывающие на одну и ту же область пространства, локально совпадали.   

### Лидар → Камера (3D → 2D)

Проекция лидара на плоскость кадра требует матрицы взаимной калибровки двух сенсоров. Не зависит от характера движения робота. Результат - изображение с глубиной, к каналам с цветом добавлено расстояние (RGBD).   
![image.png](attachments/88c2601061dbd98063ebf590bb28d740.png)    
![](attachments/4a0fe9b708eecc98a3984be716d8fbe3.png)    
Frustum PointNets for 3D Objec…    
Далеко не все пиксели изображения покрываются информацией о глубине с соответствующего луча лидара:   
![](attachments/49e7dfefebac22e4714446bdb0a6dea2.png)    

### Камера → Лидар (2D → 3D)

Ассоциация пикселей камеры с точками лидара по аналогии с примером выше, только проекция пикселей изображения в точки с трехмерными координатами, другими словами раскрашивание облака точек. К каждой лидарной точке помимо ее трех пространственных координат и интенсивности может быть добавлен цвет соответствующего пикселя, класс сегментационной маски в этом пикселе или целый вектор признаков, полученный из сверточной нейросети. Основная проблема такого метода объединения данных похожа на предыдущий пример - многие пиксели не имеют соответствующих им лидарных точек и будут просто выкинуты.   
![](attachments/cfa43f06fdea8df08b35ab90a10cd04d.png)    
PointPainting: Sequential Fusi…    

### Лидар → BEV (bird eye view)

Получение вида сверху на лидарное облако схлопыванием по высоте (y) всех лидарных точек. Далее такая плоская лидарная карта может быть преобразована в дискретную карту занятости. С подобным представлением удобно работать в плане планирования траекторий и трекинга объектов. Сложность обработки таких карт состоит в их разреженности - большинство пикселей не содержат информации.   
![](attachments/fb2f31ae27f6688e6f297f5e1e893179.png)    
autoware\_occupancy\_grid\_map\_ou…    

### Камера → BEV

Метод гомографии, репроекции изображения на вид сверху, например, используется для проекции разметки, распознанной на кадре камеры. Каждый пиксель такого спроецированного изображения лежит на плоскости на соответствующем расстоянии от камеры. Но это касается только пикселей поверхности земли, все возвышающиеся точки будут спроецированы неверно, дальше своего действительного положения (крыши машин на изображении снизу).   
![image.png](attachments/cd0d1392fce2fef228267f80c87d6f03.png)    
![](attachments/438085457c4b8dbb6f39ff78efc4df6f.png)    
В задаче детекции объектов - проецирование координат объектов на плоскость земли. В базовой постановке метод предполагает землю плоской, а ориентацию камеры относительно земли фиксированной. Проецируются только точки объекта, лежащие на земле, например нижняя грань детекционной рамки (обычно берется средняя точка этой грани, обозначающая центр объекта на земле).   
![](attachments/8d37192da9d52645655630221ce5956f.png)    
Метод хорош тем, что можно взять имеющийся хорошо обученный детектор объектов и бесплатно получить трех-мерные объекты на плоскости. Но не все так просто, каждый этап проекции накладывает свои погрешности:   

-  Погрешности калибровки камеры (можно минимизировать если постараться)   
- Погрешность неровной земли (можно минимизировать точной лидарной картой и локализацией на ней)   
- Небольшие наклоны и качения корпуса робота относительно земли (можно минимизировать, добавив информацию о положении с акселерометра)   
- Несоответствие центра объекта и его 2Д-рамки (например трамвай, который виден под углов в 45 градусов, можно уменьшить, прдсказывая маску объекта)   
- Нужно еще предсказывать размер объекта и его ориентацию (без этого никак)   
- Погрешность определения самой рамки (обычно предсказания детектора "шумят" от кадра к кадру, можно добавить трекинг и сглаживать редсказания)   
- Объекты, не стоящие на земле, спроецировать не получится, они будут предсказаны с завышенной дистанцией (светофоры, птицы и д.р.)   
- Если нижняя часть пешехода закрыта автомобилем, то рамка верхней части будет спроецирована сильно дальше того места, где пешеход стоит   
- Все ошибки нелинейно возрастают с увеличением дистанции до объекта в силу особенности проективной геометрии   
   

Как видно, нужно учесть и отшлифовать целый комплекс алгоритмических методов, чтобы такая проекция объектов с камер хорошо работала, по этому:   

> Поэтому гомография из коробки хороша для распознавания объектов, которые находятся в радиусе не дальше 40 метров от камеры. Такое поле зрения приемлемо для движения со скоростью около 20 км/ч. Этого достаточно, чтобы автономный автомобиль мог ездить по закрытой территории. [[Как эволюционировало машинное зрение автономного транспорта]] …    

