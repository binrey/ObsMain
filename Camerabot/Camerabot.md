# Робот на VLM в домашних условиях

![Для привлечения внимания](real_drive_1.gif)
## Введение

Те, кто следит за развитием ML в беспилоте, наверняка знают: в проектировании систем автономного движения существует два основных подхода — **модульный** и **сквозной (End-to-End)**. Кратко напомню их суть.

Модульный подход разделяет задачу на отдельные компоненты: детекция объектов, предсказание их движения, планирование траектории и управление. Каждый модуль оптимизируется по своей локальной метрике, что упрощает разработку и отладку, но может приводить к накоплению ошибок и потере информации между этапами обработки.

Сквозной подход объединяет все этапы в единую модель глубокого обучения, которая напрямую преобразует сырые данные сенсоров в управляющие команды. Такой подход позволяет избежать ручного проектирования признаков, обеспечивает совместную оптимизацию всех компонентов по глобальному критерию качества вождения и потенциально лучше обобщается на новые ситуации. Однако взамен мы получаем черный ящик, который сложно контролировать и тонко настраивать.

Текущее положение дел таково: на основе модульного подхода строят рабочие проекты, на основе сквозного — красивые и впечатляющие презентации. Особенно это касается настоящего end-to-end, где одна нейросеть одновременно понимает мир вокруг и планирует следующие действия. Однако [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) говорит нам, что ML-подходы всегда побеждают на длинной дистанции, выигрывая от эффекта накопления данных. А данных со временем становится только больше. Поэтому за развитием сквозного подхода стоит как минимум следить.

Какие существуют способы построения системы беспилотной езды сквозным способом?
1. Обучить агента выполнять задачу в симуляторе, а затем решить проблему переноса полученных навыков в реальный мир
2. Если данных много, обучить на них симулятор (модель мира), тогда проблема переноса знаний должна значительно уменьшиться
3. Если данных совсем нет, но хочется попробовать — использовать визуально-языковую модель (Vision Language Models, или VLM). Возможно, ее знаний хватит и для понимания мира по изображению с камеры, и для принятия решений о том, куда и как двигаться в нем.

Последний вариант — именно наш случай. Например, компания **Wayve** представила свой опыт внедрения VLM в пайплайн автопилота. Благодаря языковой части модели автопилот получил возможность не только обеспечивать езду по маршруту, но и объяснять свои действия на понятном человеку языке, что в теории позволяет приоткрыть черный ящик и понять логику принятия решений:

![wave](wave.gif)

## Архитектура

Выбор архитектуры в первую очередь определяется ограничениями, которые накладывает модель. Для управления роботом недостаточно «видеть» сцену. Модель должна одновременно понимать изображение, интерпретировать инструкцию или цель и выдавать устойчивое, формализуемое решение. Большинство быстрых vision-моделей на это не рассчитаны: они способны описывать визуальную сцену, но не действовать в роли агента.

VLM, обученные в instruct-парадигме, эту проблему решают. Они способны рассуждать и следовать системным правилам (важно, например, всегда заканчивать свой ответ командой управления из предзаданного списка). Однако за эту универсальность приходится платить вычислительной сложностью. Минимально рабочим вариантом по моим тестам будет локальная VLM начиная с 7B параметров. Для мобильного робота с ARM-процессором и жёсткими энергетическими рамками такой класс моделей оказывается практически недоступным.

Из этого ограничения логически вытекает архитектурное разделение: интеллектуальная часть выносится за пределы робота, а сам робот превращается в минималистичный исполнитель. На его стороне остаются только сенсоры и низкоуровневое управление движением. Камера выдаёт кадры, контроллер исполняет команды скорости и поворота. Вся вычислительная нагрузка концентрируется на хост-стороне. Именно там располагается VLM, принимающая изображение, инструкцию и историю прошлых состояний (при наличии), интерпретирующая ситуацию и формирующая управляющее воздействие. Хост не управляет роботом напрямую — он лишь транслирует результат рассуждения в простой и надёжный набор команд, понятный исполнительному уровню.

Это не полноценный автономный робот, но зато доступный способ быстро проверить интересные гипотезы. Цена этого подхода — задержка сети и зависимость от соединения. Для моих экспериментов было достаточно домашнего Wi-Fi, но при необходимости можно перейти на соединение по интернет-кабелю (патч-корду).

Важно подчеркнуть, что при физической распределённости система остаётся логически end-to-end. Поток данных начинается с изображения камеры и заканчивается командой движения, минуя классические промежуточные модули детекции, трекинга и планирования. VLM в этой архитектуре объединяет в себе восприятие, рассуждение, планирование и политику действий, пусть и размещённые вне корпуса робота.

Вот такой сетап использовался в эксперименте:
- ноутбук с картой RTX4080 12Гб в качестве хоста
- миникомпьютер RaspberryPi5 с широкоугольной pi-камерой на колесном ровере

На хосте и роботе развернут ROS2. Робот отправляет на хост изображения с камеры в уменьшенном до 640x480 пикс. размере. И хотя изображение передаётся в уменьшенном размере, это даёт лишь ограниченный выигрыш по задержке. В авторегрессионных VLM основное время инференса тратится на генерацию ответа: каждый следующий токен вычисляется отдельным шагом декодера. В результате именно длина ответа, а не размер кадра или промпта, становится главным фактором задержки. При выборе длины ответа важен баланс: с одной стороны хочется генерировать ответ как можно короче, в пределе только управляющую команду; с другой стороны перед окончательным решением модели нужно дать «подумать вслух».

На хосте помимо ROS2 запущена ollama с qwen2.5vl:latest — квантизованная версия на 7 миллиардов параметров. В доступе также есть версия на 3B параметров, она может работать с хорошей скоростью, почти в реальном времени, но эксперименты показали, что эта версия плохо понимает изображения. Версия latest неплохо понимает сцену и следует указаниям промпта, но работает с частотой всего около 1 fps. Это очень медленно — робот постоянно останавливается в ожидании обработки очередного кадра, но этого достаточно для тестов и демонстрации.

## Работа с моделью

Задача для модели простая — увидеть в кадре объект и направить робота к нему. Скорость оставлена фиксированной. Повороты, для упрощения, дискретные — одна команда на поворот означает смену курса на фиксированный угол.

Ключевая идея в формировании управляющего промпта — сделать его максимально простым и универсальным, дать модели максимальную свободу формирования логики управляющего сигнала, заложить как можно меньше так называемых inductive bias. Таким образом можно проверить, какие знания есть в модели и как их лучше использовать. После множества тестов близкий к оптимальному управляющий промпт выглядит так:
```
Available commands to robot:
<FORWARD> 
<RIGHT>
<LEFT> 
<BACK>

Your goal:
Using available commands to correct your direction get close to WHITE BALL as much as possible.

1. Do you see WHITE BALL on the floor?
2. Where the WHITE BALL located: near the center of a frame, close to the left side or right side of image?
3. Turn left if WHITE BALL is near the left side of image
4. Turn right if WHITE BALL is near the right side of image
5. Think before give answer. What one available command to choose to achieve the goal?
```

Полностью избежать конструкций вида «если видишь А, то делай Б» не получилось — пункты 3 и 4 жестко навязывают логику движения, а хотелось бы, чтобы нейросеть сама это придумала без подсказок. Но без этих инструкций модель слишком плохо понимала, куда нужно двигаться. Например, встречались ответы вида: «я вижу мяч, он находится справа, нужная команда — повернуть налево». Модель не объясняет детальнее логику принятия решения, что, вероятно, связано с отсутствием подобных навыков у нейросети — в обучающем датасете было очень мало подобных примеров.

Если парсер ответа нейросети находит одну из возможных команд, отправляется соответствующий управляющий сигнал роботу.

## Проверка в симуляции

Прежде чем покупать робота с камерой и миникомпьютером, полезно всё проверить в симуляторе. Webots прост и достаточно функционален. Он позволит протестировать быстродействие конкретного железа и отладить управляющий промпт. В этом случае всё происходит на хосте: модель, завернутая в ROS2-ноду, управляет виртуальным роботом в виртуальной комнате.

Ниже представлен пример такой симуляции, на которой виртуальный робот в целом успешно преследует белый мячик. Видео ускорено в 5 раз:

![webots simulation](webots.gif)

## Работа в реальном мире

Можно было остановиться на этапе симуляции, так как работа с VLM будет идентичной. Однако было интересно вывести робота в реальность. Основные изменения носят технический характер:
1. Переход на упомянутую выше схему хост-робот. Для этого на роботе нужно установить ROS2 в контейнере для ARM-процессора Raspberry Pi, настроить видимость камеры из контейнера (что оказалось непростой задачей).
2. Корректировка констант движения, так как робот отличается от симуляции и по массе, и по габаритам (вместо точного воспроизведения модели в Webots проще подстроить параметры под реальный робот). ==В серьезных беспилотных системах есть модуль контроля с обратной связью, приводящей действительную скорость к заданной в каждый момент времени. Наш робот не имеет такого модуля, поэтому он будет по-разному себя вести на разных типах покрытия.==

Важно отметить, что логика работы с VLM при переходе из симуляции в реальный мир не меняется. Благодаря значительной обобщающей способности модели белый шар в симуляции и белый мячик в реальности воспринимаются как один и тот же объект. Текстовый ответ модели не меняется при смене обстановки в кадре. Важно только, чтобы объекты были достаточно похожи, что объясняет выбор шарика в качестве цели.

Результат выполнения задачи представлен на видео в начале статьи. Ниже — еще один пример с немного измененным промптом. Изменения привели к тому, что робот стал менее уверенным в оценке расположения цели, но получил возможность найти ее при случайной потере из вида:

![real world test 2](real_drive_2.gif)




