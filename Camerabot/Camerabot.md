# Робот на VLM в домашних условиях

![Для привлечения внимания](real_drive_1.gif)
## Введение

Те, кто следит за развитием ML в беспилоте, наверняка знают: в проектировании систем автономного движения существует два основных подхода — **модульный** и **сквозной (End-to-End)**. Вкратце напомню что они значат.

Модульный подход разделяет задачу на отдельные компоненты: детекция объектов, предсказание их движения, планирование траектории и управление. Каждый модуль оптимизируется по своей локальной метрике, что упрощает разработку и отладку, но может приводить к накоплению ошибок и потере информации между этапами обработки.

Сквозной подход объединяет все этапы в единую модель глубокого обучения, которая напрямую преобразует сырые данные сенсоров в управляющие команды. Такой подход позволяет избежать ручного проектирования признаков, обеспечивает совместную оптимизацию всех компонентов по глобальному критерию качества вождения и потенциально лучше обобщается на новые ситуации. Взамен мы получаем черный ящик, который сложно контролировать и тонко настраивать.

Расклад сил между этими подходами примерно такой: на основе модульного подхода строят рабочие проекты, на основе второго — красивые и впечатляющие презентации. Особенно, если мы говорим про настоящий end-to-end, где одна нейросеть и понимает мир вокруг и планирует следующие действия. Но [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) говорит нам, что ML-подходы всегда побеждают на длинной дистанции, выигрывая от эффекта накопления данных. А данных со временем не становится меньше, только больше. По этому за развитием сквозного подхода нужно как минимум следить.

Какие есть способы построения системы беспилотной езды сквозным способом?
1. Обучить агента выполнять задачу в некотором симуляторе а дальше попытаться решить проблему переноса полученных навыков в реальный мир
2. Если данных много, то обучить на них еще и симулятор (модель мира), тогда проблема переноса знаний должна значительно кменьшиться
3. А если нет данных совсем, но попробовать хочется - давайте использовать какую-нибудь визуально-языковую модель (Vision Language Models, или VLM). Может быть ее знаний хватит и для понимания мира по изображению с камеры и для принятия решений о том куда и как двигаться в нем.

Последний вариант как раз наш. И, например, у компании **Wave** была презентация про их опыт внедрения VLM в свой пайплайн автопилота. Благодаря языковой части модели, автопилот получил возможность не только обеспечивать езду по маршруту, но и объяснять свои действия на понятном человеку языке, тем самым позволяя приоткрыть крышку того самого черного ящика и заглянуть внутрь:
![wave](wave.gif)

## Архитектура

Выбор архитектуры в первую очередь определяется ограничениями которые накладывает модель. Для управления роботом недостаточно «видеть» сцену. Модель должна одновременно понимать изображение, интерпретировать инструкцию или цель и выдавать устойчивое, формализуемое решение. Большинство быстрых vision-моделей на это не рассчитаны: они способны описывать визуальную сцену, но не действовать в роли агента.

VLM, обученные в instruct-парадигме, эту проблему решают. Они способны рассуждать и следовать системным правилам (важно, например, всегда заканчивать свой ответ командой управления из предзаданного списка). Однако за эту универсальность приходится платить вычислительной сложностью. Минимально рабочим вариантом по моим тестам будет локальная VLM начиная с 7B параметров. Для мобильного робота с ARM-процессором и жёсткими энергетическими рамками такой класс моделей оказывается практически недоступным.

Из этого ограничения логически следует архитектурное разделение: интеллектуальная часть выносится за пределы робота, а сам робот превращается в минималистичный исполнитель. На его стороне остаются только сенсоры и низкоуровневое управление движением. Камера выдаёт кадры, контроллер исполняет команды скорости и поворота. Вся вычислительная нагрузка концентрируется на хост-стороне. Именно там располагается VLM, принимающая изображение, инструкцию и историю прошлых состояний при наличии, интерпретирующая ситуацию и формирующая управляющее воздействие. Хост не управляет роботом напрямую — он лишь транслирует результат рассуждения в простой и надёжный набор команд, понятный исполнительному уровню.

Да, это не полноценный автономный робот, но зато доступный способ быстро проверить интересные гипотезы. Цена этого подхода — задержка сети и зависимость от соединения. Мне было достаточно домашнего wi-fi, но при необходимости можно вообще перейти на соединение по интернет кабелю (патч-корду).

Важно подчеркнуть, что при физической распределённости система остаётся логически end-to-end. Поток данных начинается с изображения камеры и заканчивается командой движения, минуя классические промежуточные модули детекции, трекинга и планирования. VLM в этой архитектуре объединяет в себе восприятие, рассуждение, планирование и политику действий, пусть и размещённые вне корпуса робота.

Итак, вот такой сетап был у меня под рукой:
- ноутбук с картой RTX4080 12Гб в качестве хоста
- миникомпьютер RaspberyPi5 с широкоугольной pi-камерой на колесном ровере

На хосте и роботе развернут ROS2. Робот отправляет на хост изображения с камеры в уменьшенном до 640x480 пикс. размере. И хотя изображение передаётся в уменьшенном размере, это даёт лишь ограниченный выигрыш по задержке. В авторегрессионных VLM основное время инференса тратится на генерацию ответа: каждый следующий токен вычисляется отдельным шагом декодера. В результате именно длина ответа, а не размер кадра или промпта, становится главным фактором задержки. В плане выбора длины ответа  важен баланс: с одной стороны хочется генерировать ответ как можно короче, в пределе только управляющую команду; с другой стороны перед окончательным решением модели нужно дать «подумать вслух».

На хосте помимо ROS2 запущена ollama с qwen2.5vl:latest, это квантизованная версия на 7 миллиардов параметров. В доступе так же есть 3B версия, она может работать с хорошей скоростью, почти в реальном времени, но эксперименты показали, что эта версия совсем плохо понимает изображения. Версия latest неплохо понимает сцену и следует указаниям промпта, но работает с частотой всего лишь около 1 fps. Это очень медленно, робот постоянно останавливается в ожидании обработки очередного кадра, но этого достаточно для тестов и демонстрации.

## Работа с моделью

Задача для модели будет простая - увидеть в кадре объект и направить робота ехать к нему. Скорость пока трогать не будем - пусть едет с фиксированной. Все повороты, опять же для простоты, будут дискретными - одна команда на поворот означает смену курса на какой-то фиксированный угол.

Ключевая идея в формировании управляющего промпта - сделать его максимально простым и универсальным, дать модели максимальную свободу формирования логики управляющего сигнала, заложить как можно меньше так называемых inductive bias. Таким образом можно будет проверить какие знания есть в модели и как их лучше использовать. После немалого числа тестов около оптимальный управляющий промпт выглядит так:

Available commands to robot:
<FORWARD> 
<RIGHT>
<LEFT> 
<BACK>

Your goal:
Using available commands to correct your direction get close to WHITE BALL as mutch as possible.

1. Do you see WHITE BALL on the floor?
2. Where the WHITE BALL located: near the center of a frame, close to the left side or right side of image?
3. Turn left if WHITE BALL is near the left side of image
4. Turn right if WHITE BALL is near the right side of image
5. Think before give answer. What one available command to choise to achive the goal?

 Полностью избежать конструкций вида «если видишь А, то делай Б» не проучилось - пункты 3 и 4 жестко навязывают логику движения, а хотелось бы, чтобы нейросеть сама это придумала без подсказок. ==Но без этих инструкций модель все же слишком плохо понимала куда нужно двигаться. Например можно было увидеть ответ вида: «я вижу мяч, он находится справа, нужная команда - повернуть налево». Модель не объясняет детальнее логику принятия решения, это баг, который, видимо, вытекает из отсутствия подобных навыков у нейросети, в обучающем датасете наверно было очень мало подобных примеров.== 

Далее, если парсер ответа нейросети находит одну из возможных команд, отправляется соответствующий управляющий сигнал роботу.

## Проверка в симуляции

Прежде чем бежать покупать робота с камерой и миникомпьютером полезно всё проверить в симуляторе.  Webots очень прост и достаточно функционален. Он позволит протестировать и быстродействие конкретного железа и отладить управляющий промпт. В этом случае всё будет происходить на хосте, модель, завернутая в ROS2-ноду будет управлять виртуальным роботом в виртуальной комнате.
Ниже представлен пример такой симуляции, на ней виртуальный робот в целом успешно преследует белый мячик. Видео ускорено в 3 раза:
![[видео2]]

## Работа в реальном мире

На самом деле можно было остановиться на этапе симуляции, так как работа с VLM будет идентичной. Но уж слишком интересно было вывести робота в реальность. Основные изменения будут скорее техническими:
1. Переходим на упомянутую выше схему хост-робот. Для этого на роботе нужно установить еще один ROS2 в контейнере для ARM-процессора raspberry, настроить видимость камеры из контейнера (с чем тоже оказалось много возни).
2. Немного меняем константы движения, так как робот отличается от симуляции и по массе и по габаритам (мне было лень собирать в webots как можно более точную копию реального робота, проще чуть подкрутить параметры). ==В серьезных беспилотных системах есть модуль контроля с обратной связью, приводящей действительную скорость к заданной в каждый момент времени. Наш робот не имеет такого модуля, так что он будет по разному себя вести на разных типах покрытия.==

Самое важное, что логика работы с VLM при переходе из симуляции в реальный мир никак не меняется. Благодаря значительной обобщающей способности с точки зрения модели белый шар в симуляции и белый мячик в реальности будут означать один и тот же объект. Текстовый ответ модели никак не поменяется от смены обстановки в кадре. Важно только чтобы объекты были достаточно похожи, чем объясняется мой выбор с шариком.

Результат выполнения задачи мы видели на видео в начале статьи. Ниже еще один пример с немного измененным промптом. Изменения привели к тому, что робот стал менее уверенным в оценке расположения цели, но зато получил возможность найти его при случайной потере из вида:
![[видео2]]

.


