# End-to-End беспилотник на VLM в домашних условиях. Часть 2

## Введение
Во второй части — практическая часть проекта: установка и первый запуск, эксперименты по созданию идеального промпта, а также попытки добавить «память» о прошлом шаге. Промпт теперь не содержит жестких инструкций поведения и работает в полтора раза быстрее! В конце переходим от симуляции к реальному роверу: сборка, настройка и проверка работы на железе.

## Установка и первый запуск на хосте
Установка происходит просто, командой `./host.sh` устанавливается образ и запускается контейнер с Ubuntu 22.04 и ROS2 Humble. Далее в созданном контейнере нужно собрать пакеты `./b` и активировать виртуальное окружение `. e`. 
Проверить работу симулятора и его связь с ROS2 можно следующим образом:
```
ros2 launch robot sim_teleop.py
```
Запустится симуляция и подключится управление с помощью клавиатуры.

За контроль робота отвечает универсальный класс BaseDriver, который будет отправлять команды как виртуальному роботу так и реальному, только с разными параметрами. Как было сказано в первой части, управление осуществляется командами, задающими постоянную скорость. Ее величину нужно подобрать под конкретного робота и тип поверхности. Самое главное - это сбалансировать скорость движения и скорость обработки кадра, чтобы робот не успевал уехать слишком далеко пока ждет ответа на прошлый кадр. Если не получается достаточно уменьшить скорость (робот просто перестает двигаться), ограничиваем максимальное число тактов в ожидании команды `max_steps_without_command`.

## Управление с VLM
### Подготовка
Теперь настало время заменить ручное управление на команды визуально-языковой модели (VLM). Сперва нужно установить ollama и скачать нужную модель: `ollama run qwen2.5vl:latest`. В последующие разы скачанная модель будет запускаться автоматически при старте автопилота. Как было сказано в первой части, оптимальным выбором стал **qwen2.5vl:latest** (7b), он стоит в launch-файлах по умолчанию.

За отправку запросов к ollama и обработку её сообщений отвечает нода vlm_control. В файле **resourses/vlm_prompt.txt** записана управляющая инструкция или промпт, специально подобранный для 7b модели. Он загружается при старте ноды и его можно легко редактировать. Промпт вместе с кадром отправляется в модель, из полученного ответа извлекается выбранная моделью команда управления (если их несколько, то берется последняя), команда отправляется роботу.

Чтобы показать, почему итоговый вариант промпта именно такой, мы рассмотрим еще несколько его вариантов, начиная с самого простого, постепенно добавляя необходимые ингридиенты.

### Управление в один токен
Как известно, в авторегрессионных VLM основное время инференса тратится на генерацию ответа: каждый следующий токен вычисляется отдельным шагом декодера. В результате именно длина ответа, а не размер кадра или промпта, становится главным фактором задержки. По этой причине первое желание - генерировать ответ как можно короче, в пределе только управляющую команду. Попробуем так сделать и с нашей моделью:
```
You control a wheel robot.
Available commands to robot:
<FORWARD> - slowly go forward
<RIGHT> - turn right without moving
<LEFT> - turn left without moving
<BACK> - slowly go back
<STOP> - stay

Your goal:
Using available commands to correct your direction get close to WHITE BALL as much as possible and touch it.

What available command to choose to achieve the goal? Answer in short!
```
На симуляции, показанной ниже, видно, что в плане скорости удалось добиться поставленной цели - время обработки кадра хотя бы меньше секунды. Однако ответы модели совершенно бессвязные:

![single_prompt](single_prompt.gif)

### Цепочка рассуждений
А вот если позволить модели немного «подумать», можно получить более осознанное поведение. Нужно просто убрать из промпта требование отвечать коротко:
```
You control a wheel robot.
Available commands to robot:
<FORWARD> - slowly go forward
<RIGHT> - turn right without moving
<LEFT> - turn left without moving
<BACK> - slowly go back
<STOP> - stay

Your goal:
Using available commands to correct your direction get close to WHITE BALL as much as possible and touch it.

What available command to choose to achieve the goal?
```
Избавившись от навязанных ограничений,  модель qwen2.5vl начинает сама создавать цепочки рассуждений в процессе генерации ответа:

![chan_of_thoughts](coc.gif)

Ответы становятся заметно длиннее, существенно снижается скорость реакции. Но хорошо заметно, что у робота уже что-то начинает получаться. Он «осознает» происходящее и пытается справится с задачей…но можно лучше!

### Промпт с наставлением
Здесь будет приведен финальный вариант промпта, который очень хорошо работает для нашей задачи:
```
You control a wheel robot.
Available commands to robot:
<FORWARD> - slowly go forward
<RIGHT> - turn right without moving
<LEFT> - turn left without moving
<BACK> - slowly go back
<STOP> - stay

Your goal:
Using available commands to correct your direction get close to WHITE BALL as much as possible and touch it.

- Describe in new line started with ???: Where WHITE BALL located now relative to robot and what approximate distance to it?
- If you don't see WHITE BALL - move back.
- Answer in new line started with >>>: What available command to choose to achieve the goal?
```

Основное отличие - указание перед тем как дать ответ кратко описать сцену. Знаки вопросов при этом выполняют чисто техническую функцию, о которой будет рассказано ниже. Подсказка двигаться назад в случае если цели не видно - вспомогательный, но очень полезный функционал. Без него тоже работает, но при потере цели из виду робот просто останавливается. Символ «>>>» и дополнительные указания начинать с новой строки так же не являются обязательными, случайно или нет, с ними работает чуть стабильнее и быстрее. Почему? Это та самая необъяснимая «магия» промптинга. Каждое слово или символ могут полностью сломать поведение модели без каких либо очевидных причин. Возможно, такая нестабильность - следствие попыток использовать очень короткие промпты, оптимальные с точки зрения скорости обработки и точности. Возможно я просто забыл в конце добавить слово «пожалуйста». 

Но зато, посмотрите как четко и достаточно быстро робот справляется с задачей:
![main_prompt](coc_with_rules.gif)

Ответы компактные, но содержат всё необходимое для принятия правильного решения. Сделать более наглядный и системный «ablation study”, выяснить влияние каждой строчки и формулировки  не представлялось возможным по описанным выше причинам - слишком много шума в промптинге.

В следующем абзаце сделана моя интерпретация проведённых экспериментов. Не претендуя на высоко экспертное объяснение, изложу своё видение cv-инженера, делающего первые шаги в мир визуально-языковых моделей.

### Объяснительная

Проведённые эксперименты - это наглядная демонстрация сразу двух тесно связанных феноменов: обучения по контексту (in-context learning, ICL) и цепочки рассуждений (chain of thought, CoT).

Иными словами, у универсальной большой VLM нет явно выученной функции вида «картинка → команда», как это бывает в специализированных end-to-end моделях управления. Зато в процессе обучения в её параметрах закрепились более общие и декомпозированные знания, распределённые между модальностями:
- в визуальной части — навыки восприятия, например задачи вида «обнаружь объект на изображении», «определи его положение в кадре», «опиши сцену»;
- в языковой части — абстрактные правила и причинно-следственные зависимости, такие как «если цель находится слева — следует повернуть налево», «если это препятствие — лучше объехать его справа».

Когда мы заставляем модель отвечать на эти вопросы последовательно, мы фактически вынуждаем её разложить исходную задачу управления на несколько более простых подзадач: сначала восприятие и интерпретация сцены, затем — выбор действия на основе описанных правил. Соответствующие знания из визуальной и языковой подсистем не используются напрямую, а извлекаются в виде сгенерированных токенов, которые помещаются в общий контекст модели. 

Далее, благодаря авторегрессионной природе генерации, эти токены начинают влиять на последующие шаги вывода. Модель использует собственные промежуточные результаты как входные данные для следующих предсказаний и тем самым совместно обрабатывает визуальную информацию и логические правила. В результате сложная задача «восприятие → решение → действие» сводится к последовательному решению элементарных шагов — предсказаний следующего токена.

#### Добавление информации о прошлом состоянии
До сих пор оставалась недореализованной одна «тудушка» из первой части - грамотное подключение истории состояний робота в контекст промпта. Еще один подход к снаряду привел к более-менее рабочему варианту. 

Неожиданная трудность, с которой пришлось столкнуться состояла в том, что при наивной реализации, когда весь предыдущий ответ модели или даже несколько таких подгружались прямо в промт, это приводило к полной потери работоспособности, константному ответу при любой картинке на входе. Модель как будто переставала понимать, что от нее просят. Даже при специальных выделениях части с историей состояний и указаний на нее в промпте.

Решение простое - брать только самую нужную информацию: прошлое описание сцены и выбранную команду. С командой всё просто - её мы и так сами выбрали перед отправкой к роботу. Будем добавлять её в начало исходного промпта на каждом шаге с подписью, чтобы модель поняла что там ей пишут:
```
if self.add_last_command:
    prompt = f"Last step command: {self.history[-1]['last_command']}" + "\n" + prompt
```
Результат примерно такой: информация только об одной принятой моделью прошлой командой особо ей не помогает в принятии решений, а в текущей реализации даже мешает - появились ненужные остановки. Почему изначальное поведение так сильно ломается сказать не могу, были опробованы разные несложные правки, в том числе попросить модель проговорить самой свое прошлое движение, чтобы потенциально лучше его обработать в цепочке рассуждений.
![main_prompt_with_last_cmd](last_cmd.gif)
Чтобы выделить в ответе модели описание сцены на прошлом шаге простой эвристики недостаточно. К счастью можно просто попросить модель сделать это самой в рамках её же ответа. В промпте для этого есть такой спецсимвол «???» (см. **parse_scene_description**). Краткое описание предыдущей сцены так же подается в начало промпта со специальной пометкой:
```
if self.add_last_scene:
    prompt = f"Last step scene: {self.history[-1]['last_scene']}" + "\n" + prompt
```
Любопытно, но эффект еще хуже чем при добавлении последней команды, робот просто не может думать:
![main_prompt_with_last_cmd_scene](last_cmd_scene.gif)
Причина мне не совсем ясна, казалось бы дополнительная информация в любом виде должна помогать принимать правильные решения. Учитывая то, насколько непредсказуемым бывают те или иные правки в промпте, можно предположить, что при должном терпении и здесь можно найти рабочий вариант.

## Сборка робота
В качестве платформы мной была выбран [Waveshare 4WD rover](https://www.waveshare.com/product/robotics/wave-rover.htm?___SID=U). Классно выглядит, неплохо ездит, батарея вроде неплохая. Возможно лучше поискать что-то еще проще, потому что этот ровер, например, всегда держит wifi точку доступа с одним открытым адресом для управления им через браузер. А оно нам вообще не нужно.
[Широкоугольная камера](https://www.waveshare.com/ov9281-160-camera.htm) имеет угол обзора в 126 град. и это то что нужно, меньше не берите. Для таких камер часто надо докупать [Шлейф](https://ozon.ru/t/39Rgc1N), потому что в RaspberryPi5 уменьшили ширину гнезда для шлейфа камеры. [Радиатор](https://ozon.ru/t/NIfLVYG) лучше брать родной, он не загораживает собой отверстия для монтажа платы.

## Настройка робота
Настройка самого робота будет зависеть от конкретной модели. Например, для waveshare ровера нужно вручную отключить запускаемый при старте процесс, блокирующий управляющую плату.

Настройка RaspberryPi начинается с установки на нее raspberryOS. Я ставил bookworm версию, на всякий случай сохранил образ тут. После этого в домашнюю директорию нужно склонировать репозиторий и установить докер-контейнер с ROS2:
`
bash robot.sh
`
Первый запуск будет долгим, потому что скрипт пойдет настраивать драйвера для pi-камеры. Драевера легко ставятся в нативную raspberryOS, а вот в контейнер с обычной ubuntu сделать это оказалось непросто. Приходится собирать на месте из исходников. После завершения установки останется собрать пакет robot и активировать среду двумя быстрыми командами:
`
./b
. e
`
По умолчанию скрипт создает контейнер без проброса X. Если RaspberryPi хочется подключить к монитору, то нужно немного поменять robot.sh:
``
Чтобы проверить работу камеры можно воспользоваться скриптом:
`python3 tools/picamera_test.py`

Далее можно потестировать, что робот и хост видят друг-друга:
- на хосте: `ros2 run robot listener`
- на роботе: `ros2 run robot talker`

Если в консоли робота видны сообщения с хоста, то всё хорошо, можно поуправлять кнопками роботом с камерой.
На хосте:
- консоль 1: `ros2 launch robot keyboard teleop`
- консоль 2: `rqt`
На роботе:
- терминал 1: `ros2 run robot camera`
- терминал 2: `ros2 launch robot ugv_control`

## Запуск робота с VLM
Если все описанные выше шаги были успешно пройдены, то ровер поедет. 
