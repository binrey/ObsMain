[Waveshare 4WD rover](https://www.waveshare.com/product/robotics/wave-rover.htm?___SID=U)
[Широкоугольная камера](https://www.waveshare.com/ov9281-160-camera.htm)
[Шлейф](https://ozon.ru/t/39Rgc1N)
[Радиатор](https://ozon.ru/t/NIfLVYG)

## Установка и первый запуск на хост-машине
Установка происходит просто, командой `./host.sh` устанавливается образ и запускается контейнер с Ubuntu 22.04 и ROS2 Humble. Далее в созданном контейнере нужно собрать пакеты `./b` и активировать виртуальное окружение `. e`. 
Проверить работу симулятора и его связь с ROS2 можно следующим образом:
`ros2 launch robot sim_teleop.py`
При этом запускается симуляция и подключается управление с помощью клавиатуры.
За контроль робота отвечает универсальная нода base_control, которая будет управлять как виртуалтным роботом так и реальным только с разными параметрами. Как было сказано в первой части, управление осуществляется командами, задающими постоянную скорость. Ее величину нужно подобрать под конкретного робота и тип поверхности. Самое главное - это сбалансировать скорость движения и скорость обработки кадра, чтобы робот не успевал уехать слишком далеко пока ждет ответа на прошлый кадр. Если не получается достаточно уменьшить скорость (робот просто перестает двигаться), полезно ограничить максимальное число тактов в ожидании команды. Именно так пришлось сделать с моим роботом, что объясняет его движение рывками.

## Управление с VLM
Теперь настало время заменить ручное управление на команды визуально-языковой модели (VLM). Сперва нужно установить ollama и скачать нужную модель `ollama serve qwen2.5vl:latest`. В последующие разы скачанная модель будет запускаться автоматически при обращении. Как было сказано в первой части, оптимальным выбором стал qwen2.5vl:latest (7b), он стоит в launch-файлах по умолчанию. Далее по ходу статьи для наглядности будут приведены результаты использования 3b и 32b моделей, но пока разберемся с промптом, а все дальнейшие тесты будут сделаны на версии 7b. 

За отправку запросов к ollama и обработку её сообщений отвечает нода vlm_control. В файле **resourses/vlm_prompt.txt** записана управляющая инструкция или промпт, специально подобранный для 7b модели. Он загружается при старте ноды и его можно легко редактировать. Чтобы показать, почему итоговый вариант промпта именно такой, мы рассмотрим еще несколько его вариантов, начиная с самой простой версии.

#### Управление в один токен
Как известно, в авторегрессионных VLM основное время инференса тратится на генерацию ответа: каждый следующий токен вычисляется отдельным шагом декодера. В результате именно длина ответа, а не размер кадра или промпта, становится главным фактором задержки. По этой причине хочется генерировать ответ как можно короче, в пределе только управляющую команду. Попробуем так сделать и с нашей моделью:
`
Prompt
`
На симуляции, показанной ниже, видно, что в плане скорости удалось добиться поставленной цели - время обработки кадра не превышает заданный таймаут в 2 сек. и робот плавно движется без прерываний. Однако ответы модели совершенно бессвязные.

#### А если подумать
Итак, мы подошли к самому интересному моменту — тому, который лично меня волновал больше всего. Оказывается, если всего лишь добавить в промпт несколько наводящих вопросов и позволить модели немного «подумать»*, можно получить осознанное и вполне успешное поведение.
`
prompt
`
Это наглядная демонстрация сразу двух тесно связанных феноменов: обучения по контексту (in-context learning, ICL) и цепочки рассуждений (chain of thought, CoT).

Иными словами, у универсальной большой VLM нет явно выученной функции вида «картинка → команда», как это бывает в специализированных end-to-end моделях управления. Зато в процессе обучения в её параметрах закрепились более общие и декомпозированные знания, распределённые между модальностями:
- в визуальной части — навыки восприятия, например задачи вида «обнаружь объект на изображении», «определи его положение в кадре», «опиши сцену»;
- в языковой части — абстрактные правила и причинно-следственные зависимости, такие как «если цель находится слева — следует повернуть налево», «если впереди препятствие — лучше объехать его справа».

Когда мы заставляем модель отвечать на эти вопросы последовательно, мы фактически вынуждаем её разложить исходную задачу управления на несколько более простых подзадач: сначала восприятие и интерпретация сцены, затем — выбор действия на основе описанных правил. Соответствующие знания из визуальной и языковой подсистем не используются напрямую, а извлекаются в виде сгенерированных токенов, которые помещаются в общий контекст модели. 

Далее, благодаря авторегрессионной природе генерации, эти токены начинают влиять на последующие шаги вывода. Модель использует собственные промежуточные результаты как входные данные для следующих предсказаний и тем самым совместно обрабатывает визуальную информацию и логические правила. В результате сложная задача «восприятие → решение → действие» сводится к последовательному решению элементарных шагов — предсказаний следующего токена.

#### Добавление прошлого состояния
До сих пор оставалась недореализованной одна тудушка из первой части - грамотное подключение истории состояний робота в контекст промпта. Еще один подход к снаряду привел к более-менее рабочему варианту. 

Неожиданная трудность, с которой пришлось столкнуться состояла в том, что при наивной реализации, когда весь предыдущий ответ модели или даже несколько таких подгружались прямо в промт, это приводило к полной потери работоспособности, константному ответу при любой картинке на входе. Модель как будто переставала понимать, что от нее просят. Даже при специальных выделениях части с историей состояний и указаний на нее в промпте.

Решение было найдено простое - брать только самую нужную информацию: прошлое описание сцены и выбранную команду. Если с командой всё просто, её мы и так сами выбрали перед отправкой к роботу, то чтобы выделить в ответе модели описание сцены простой эвристики недостаточно. К счастью можно просто попросить модель сделать это самой в рамках её же ответа:
`
prompt
`

## Настройка робота
Настройка самого робота будет зависеть от конкретной модели. Например, для waveshare ровера нужно вручную отключить запускаемый при старте процесс, блокирующий управляющую плату.

Настройка RaspberryPi начинается с установки на нее raspberryOS. Я ставил bookworm версию, на всякий случай сохранил образ тут. После этого в домашнюю директорию нужно склонировать репозиторий и установить докер-контейнер с ROS2:
`
bash robot.sh
`
Первый запуск будет долгим, потому что скрипт пойдет настраивать драйвера для pi-камеры. Драевера легко ставятся в нативную raspberryOS, а вот в контейнер с обычной ubuntu сделать это оказалось непросто. Приходится собирать на месте из исходников. После завершения установки останется собрать пакет robot и активировать среду двумя быстрыми командами:
`
./b
. e
`
По умолчанию скрипт создает контейнер без проброса X. Если RaspberryPi хочется подключить к монитору, то нужно немного поменять robot.sh:
``
Чтобы проверить работу камеры можно воспользоваться скриптом:
`python3 tools/picamera_test.py`

Далее можно потестировать, что робот и хост-компьютер видят друг-друга:
- на хосте: `ros2 run robot listener`
- на роботе: `ros2 run robot talker`

Если в консоле робота видны сообщения с хоста, то всё хорошо, можно поуправлять кнопками роботом с камерой.
На хосте:
- консоль 1: `ros2 launch robot keyboard teleop`
- консоль 2: `rqt`
На роботе:
- терминал 1: `ros2 run robot camera`
- терминал 2: `ros2 launch robot ugv_control`

## Запуск робота с VLM
