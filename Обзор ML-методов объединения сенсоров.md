---
tags:
  - Sensor_Fusion
type: page
---
---

## [PointPainting](https://github.com/Song-Jingyu/PointPainting?tab=readme-ov-file)   
Добавление к каждой лидарной точке информации о классе, полученной из спроецированной сегментационной маски. Маски получаются от любой 2Д-модели семантической или инстанс сегментации (в данном случае DeepLav-V2). В качестве 3Д-детектора можно использовать текущий центерпоинт, требуются минимальные изменения.    
- возможность работы с многими камерами "из коробки".   
![](attachments/fb2f31ae27f6688e6f297f5e1e893179.png)    
- больше всего улучшает качество распознавания маленьких объектов   
![](attachments/873a85afc7a44794f21bd3d07738ec24.png)    
- Метод может работать на одном лидаре или на синхронных лидарных и камерных данных   
- 2Д и 3Д модели можно обучать отдельно на не синхронных данных   
   
[PointPainting: Sequential Fusi…](PointPainting.%20Sequential%20Fusion%20for%203D%20Object%20Detection.md)    
## [F-PointNet](https://arxiv.org/pdf/1711.08488)   
Использование 2Д-детектора в качестве источника регионов интереса для последующей классификации и определения ориентации по лидарным точкам с помощью PointNet.    

- Метод может работать только на лидарных и камереных данных   
- 2Д и 3Д модели можно обучать отдельно на не синхронных данных   
   
[Frustum PointNets for 3D Objec…](Frustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data.md)    
## [LaserNet++](https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.pdf)   
Объединение признаков с камеры с проекцией лидара на плоскость кадра. Все дальнейщие вычисления происходять по принципу обработки 2Д-изображений. Метод лучше всего работает если лидар и камера находятся близко друг к другу, в этом случае "тени" от объектов на лидарном облаке не будут видны с ракурса камеры.   
![](attachments/36bdf1df80bfa17879e42d4d8e877ff1.png)    
- Метод может работать на одном лидаре или на синхронных лидарных и камерных данных   
- Модель нужно обучать  на синхронных данных   
   
 [LaserNet++: Sensor Fusion for …](LaserNet++.%20Sensor%20Fusion%20for%20Joint%203D%20Object%20Detection%20and%20Semantic%20Segmentation.md)    
## Фьюзинг в Яндексе   
Судя по доступной информации основан на идее LaserNet++. Только в отличие от последнего, признаки на выходе из сверточного энкодера с помощью информации о расстояниях до лидарных точек проецируются обратно на вид сверху. Там с одной стороны строится карта занятости для статичных объектов и по аналогии с CenterPoint вычисляются положения и характеристики 3Д-объектов.    
![](attachments/6cb8507e613278de379ae20b28f22405.png)    
- Метод может работать на одном лидаре или на синхронных лидарных и камерных данных   
- Модель нужно обучать на синхронных данных, для ускорения обучения можно использовать свои уже имеющиеся предобученные 2D-модели.   
   
[ГорынычNet](ГорынычNet.md)    
## [TransFusion](https://github.com/XuyangBai/TransFusion/)   
Трансформенный декодер, использующий сверточный энкодер для интеграции признаков изображения. Интересные особенности:    
- механизм, повышающий устойчивость к ошибкам калибровки   
- повышение полноты детектирования объектов за счет использования признаков камеры   
- трекинг лучше чем у CenterPoint   
- улучшение распознавания объектов вдали и ночное время    
- возможность работы с многими камерами "из коробки"   
- есть TensorRT реализация   
![](attachments/27b6d59882969e05629f59ed4f1f51ad.png)    
- Метод может работать на одном лидаре или на синхронных лидарных и камерных данных   
- Модель нужно обучать на синхронных данных, для ускорения обучения можно использовать свои уже имеющиеся предобученные 2D- и 3D-модели.   
   
[TransFusion: Robust LiDAR-Came…](TransFusion.%20Robust%20LiDAR-Camera%20Fusion%20for%203D%20Object%20Detection%20with%20Transformers.md)    
## [BEVFusion](BEVFusion.%20Multi-Task%20Multi-Sensor%20Fusion%20with%20Unified%20Bird’s-Eye%20View%20Representation.md)   
Объединение признаков изображения и лидара в общем пространстве - проекции на вид сверху. Изображение проецируется путем предсказания карты глубины (2D → BEV) за счет чего метод может работать только по данным с камеры   
- заявляется возможность легкого добавления радарных данных и функции трекинга   
- повышение полноты детектирования объектов (по сравнению с CenterPoint) за счет использования признаков камеры   
- улучшение распознавания объектов вдали и ночное время  (по сравнению с CenterPoint)   
- возможность работы с многими камерами "из коробки"   
- возможность обучить задачу сегментации "из коробки"   
- есть TensorRT реализация   
![[attachments/image_r 1.png]]    
- Метод может работать только на лидаре, только на камере или на синхронных лидарных и камерных данных   
- Модель нужно обучать на синхронных данных, для ускорения обучения можно использовать свои уже имеющиеся предобученные 2D- и 3D-модели.   
   
[BEVFusion: Multi-Task Multi-Se…](BEVFusion.%20Multi-Task%20Multi-Sensor%20Fusion%20with%20Unified%20Bird’s-Eye%20View%20Representation.md)    
